{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed8921b3-2eca-439d-8bca-026cdac40f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Intialize the SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856a4457-6c17-4824-912d-947eba643f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3525126442407722#setting/sparkui/0611-043339-3vb7b9iv/driver-5079266641301177846\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ae911904750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    "      .appName(\"Combining-data\")\\\n",
    "      .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9d13f1-ed18-49fc-a48c-19a1a5dcde91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**create the Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b14334-2a28-49a4-9cc1-79026e85886f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n+------+-----------+------+\n\n+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n+------+----+------+\n\n+------+----------------+-----------+\n|  Name|         Project|HoursWorked|\n+------+----------------+-----------+\n|Ananya|       HR Portal|        120|\n| Rahul|   Data Platform|        200|\n| Priya|   Data Platform|        180|\n|  Zoya|Campaign Tracker|        100|\n| Karan|       HR Portal|        130|\n|Naveen|     ML Pipeline|        220|\n|Fatima|Campaign Tracker|         90|\n+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Employee Data\n",
    "employee_data = [ \n",
    "    (\"Ananya\", \"HR\", 52000), \n",
    "    (\"Rahul\", \"Engineering\", 65000), \n",
    "    (\"Priya\", \"Engineering\", 60000), \n",
    "    (\"Zoya\", \"Marketing\", 48000), \n",
    "    (\"Karan\", \"HR\", 53000), \n",
    "    (\"Naveen\", \"Engineering\", 70000), \n",
    "    (\"Fatima\", \"Marketing\", 45000) \n",
    "]\n",
    "columns_emp = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(employee_data, columns_emp)\n",
    "# Performance Data\n",
    "performance_data = [ \n",
    "    (\"Ananya\", 2023, 4.5), \n",
    "    (\"Rahul\", 2023, 4.9), \n",
    "    (\"Priya\", 2023, 4.3), \n",
    "    (\"Zoya\", 2023, 3.8), \n",
    "    (\"Karan\", 2023, 4.1), \n",
    "    (\"Naveen\", 2023, 4.7), \n",
    "    (\"Fatima\", 2023, 3.9) \n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance_data, columns_perf)\n",
    "# Project Data\n",
    "project_data = [ \n",
    "    (\"Ananya\", \"HR Portal\", 120), \n",
    "    (\"Rahul\", \"Data Platform\", 200), \n",
    "    (\"Priya\", \"Data Platform\", 180), \n",
    "    (\"Zoya\", \"Campaign Tracker\", 100), \n",
    "    (\"Karan\", \"HR Portal\", 130), \n",
    "    (\"Naveen\", \"ML Pipeline\", 220), \n",
    "    (\"Fatima\", \"Campaign Tracker\", 90) \n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)\n",
    "df_emp.show()\n",
    "df_perf.show()\n",
    "df_proj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7373cc8-4d2a-461c-8964-3305252dfc89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Join all three DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25201fbe-1a29-4573-90ef-b84a579cc0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined DataFrames:\n+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\nTotal hours worked per department:\n+-----------+----------+\n| Department|TotalHours|\n+-----------+----------+\n|         HR|       250|\n|Engineering|       600|\n|  Marketing|       190|\n+-----------+----------+\n\nAverage rating per project:\n+----------------+------------------+\n|         Project|         AvgRating|\n+----------------+------------------+\n|       HR Portal|               4.3|\n|   Data Platform|               4.6|\n|Campaign Tracker|3.8499999999999996|\n|     ML Pipeline|               4.7|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#1.Join all three DataFrames\n",
    "print(\"Joined DataFrames:\")\n",
    "joined = df_emp.join(df_perf, \"Name\", \"inner\") \\\n",
    "                  .join(df_proj, \"Name\", \"inner\")\n",
    "joined.show()\n",
    "#2.Compute total hours worked per department\n",
    "from pyspark.sql.functions import sum\n",
    "print(\"Total hours worked per department:\")\n",
    "joined.groupBy(\"Department\").agg(sum(\"HoursWorked\").alias(\"TotalHours\")).show()\n",
    "#3.Compute average rating per project\n",
    "from pyspark.sql.functions import avg\n",
    "print(\"Average rating per project:\")\n",
    "joined.groupBy(\"Project\").agg(avg(\"Rating\").alias(\"AvgRating\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ae50c82-e95c-461e-957c-e0788f03db4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Handling Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200f7efa-2641-4bd1-8b12-1ba15864d430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame with null rating:\n+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n|Deepak|         HR| 50000|2023|  NULL|       HR Portal|        100|\n+------+-----------+------+----+------+----------------+-----------+\n\nFiltered rows with null ratings:\n+------+----------+------+----+------+---------+-----------+\n|  Name|Department|Salary|Year|Rating|  Project|HoursWorked|\n+------+----------+------+----+------+---------+-----------+\n|Deepak|        HR| 50000|2023|  NULL|HR Portal|        100|\n+------+----------+------+----+------+---------+-----------+\n\nReplacing null ratings with department average\n+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Deepak|         HR| 50000|2023|   4.3|       HR Portal|        100|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "# 4. Add a row with null rating\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Rating\", DoubleType(), True),  \n",
    "    StructField(\"Project\", StringType(), True),\n",
    "    StructField(\"HoursWorked\", IntegerType(), True)\n",
    "])\n",
    "# Create the new row as a DataFrame with schema\n",
    "new_data = [(\"Deepak\", \"HR\", 50000, 2023, None, \"HR Portal\", 100)]\n",
    "new_df = spark.createDataFrame(new_data, schema)\n",
    "df_with_null = joined.unionByName(new_df)\n",
    "print(\"New DataFrame with null rating:\")\n",
    "df_with_null.show()\n",
    "# 5. Filter rows with null values\n",
    "from pyspark.sql.functions import col\n",
    "print(\"Filtered rows with null ratings:\")\n",
    "df_with_null.filter(col(\"Rating\").isNull()).show()\n",
    "# 6. Replace null ratings with department average\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, when\n",
    "print(\"Replacing null ratings with department average\")\n",
    "window_spec = Window.partitionBy(\"Department\")\n",
    "df_filled = df_with_null.withColumn(\n",
    "    \"Rating\",\n",
    "    when(col(\"Rating\").isNull(), avg(\"Rating\").over(window_spec)).otherwise(col(\"Rating\"))\n",
    ")\n",
    "df_filled.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fb6c988-78a7-466f-8608-16963e50bdde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Built-In Functions and UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffa45fd-b167-46e2-aa0a-e8d94320c366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerformanceCategory column:\n+------+-----------+------+----+------+----------------+-----------+-------------------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|\n+------+-----------+------+----+------+----------------+-----------+-------------------+\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|               Good|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|          Excellent|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|               Good|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|               Good|\n|Deepak|         HR| 50000|2023|   4.3|       HR Portal|        100|               Good|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|            Average|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|            Average|\n+------+-----------+------+----+------+----------------+-----------+-------------------+\n\nBonus column:\n+------+-----------+------+----+------+----------------+-----------+-------------------+-----+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|Bonus|\n+------+-----------+------+----+------+----------------+-----------+-------------------+-----+\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|               Good| 5000|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|          Excellent| 5000|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|10000|\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|               Good| 5000|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|               Good| 5000|\n|Deepak|         HR| 50000|2023|   4.3|       HR Portal|        100|               Good| 5000|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|            Average| 5000|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|            Average| 5000|\n+------+-----------+------+----+------+----------------+-----------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#7.Create PerformanceCategory column\n",
    "df_filled = df_filled.withColumn(\n",
    "    \"PerformanceCategory\",\n",
    "    when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
    "    .when(col(\"Rating\") >= 4.0, \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "print(\"PerformanceCategory column:\")\n",
    "df_filled.show()\n",
    "#8.Create UDF to assign bonus\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def bonus(hours):\n",
    "    return 10000 if hours > 200 else 5000\n",
    "bonus_udf = udf(bonus, IntegerType())\n",
    "df_filled = df_filled.withColumn(\"Bonus\", bonus_udf(col(\"HoursWorked\")))\n",
    "print(\"Bonus column:\")\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90420c0f-e9ee-4764-9dd0-1831afb8599e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Date and Time Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c82e27-6324-4731-94de-8010267c84ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JoinDate and MonthsWorked columns:\n+------+-----------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|Bonus|  JoinDate|MonthsWorked|\n+------+-----------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|               Good| 5000|2021-06-01| 48.32258065|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|          Excellent| 5000|2021-06-01| 48.32258065|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|10000|2021-06-01| 48.32258065|\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|               Good| 5000|2021-06-01| 48.32258065|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|               Good| 5000|2021-06-01| 48.32258065|\n|Deepak|         HR| 50000|2023|   4.3|       HR Portal|        100|               Good| 5000|2021-06-01| 48.32258065|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|            Average| 5000|2021-06-01| 48.32258065|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|            Average| 5000|2021-06-01| 48.32258065|\n+------+-----------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+\n\nEmployees who joined before 2022:\n+------+----------+\n|  Name|  JoinDate|\n+------+----------+\n|Ananya|2021-06-01|\n| Priya|2021-06-01|\n| Rahul|2021-06-01|\n|  Zoya|2021-06-01|\n| Karan|2021-06-01|\n|Naveen|2021-06-01|\n|Fatima|2021-06-01|\n|Deepak|2021-06-01|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#9.Add JoinDate and MonthsWorked\n",
    "from pyspark.sql.functions import lit, months_between, current_date, to_date\n",
    "df_filled = df_filled.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\"))) \\\n",
    "                     .withColumn(\"MonthsWorked\", months_between(current_date(), col(\"JoinDate\")))\n",
    "print(\"JoinDate and MonthsWorked columns:\")\n",
    "df_filled.show()\n",
    "#10.Employees who joined before 2022\n",
    "print(\"Employees who joined before 2022:\")\n",
    "df_filled.filter(col(\"JoinDate\") < \"2022-01-01\").select(\"Name\", \"JoinDate\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847a5db7-ce17-48e2-a14b-cd19b979c034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e701b30-d498-40e4-92b2-b1ac916993d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All employees:\n+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Meena|         HR| 48000|\n|   Raj|  Marketing| 51000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#11.Union with extra employees\n",
    "extra_employees = [(\"Meena\", \"HR\", 48000), (\"Raj\", \"Marketing\", 51000)]\n",
    "df_extra = spark.createDataFrame(extra_employees, [\"Name\", \"Department\", \"Salary\"])\n",
    "all_employees = df_emp.unionByName(df_extra)\n",
    "print(\"All employees:\")\n",
    "all_employees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6e7530-9b0a-41b0-a597-3760d4f38733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Saving Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0730afb8-afa2-4fd8-b859-dc91664ab09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#11.Save final dataset as partitioned Parquet by Department\n",
    "df_filled.write.partitionBy(\"Department\").mode(\"overwrite\").parquet(\"/FileStore/final_employee_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6f23d0-d533-40c6-9ac6-15dd304091b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+-----------+\n|  Name|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|Bonus|  JoinDate|MonthsWorked| Department|\n+------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+-----------+\n|Naveen| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|10000|2021-06-01| 48.32258065|Engineering|\n| Rahul| 65000|2023|   4.9|   Data Platform|        200|          Excellent| 5000|2021-06-01| 48.32258065|Engineering|\n| Priya| 60000|2023|   4.3|   Data Platform|        180|               Good| 5000|2021-06-01| 48.32258065|Engineering|\n|Fatima| 45000|2023|   3.9|Campaign Tracker|         90|            Average| 5000|2021-06-01| 48.32258065|  Marketing|\n|  Zoya| 48000|2023|   3.8|Campaign Tracker|        100|            Average| 5000|2021-06-01| 48.32258065|  Marketing|\n|Deepak| 50000|2023|   4.3|       HR Portal|        100|               Good| 5000|2021-06-01| 48.32258065|         HR|\n| Karan| 53000|2023|   4.1|       HR Portal|        130|               Good| 5000|2021-06-01| 48.32258065|         HR|\n|Ananya| 52000|2023|   4.5|       HR Portal|        120|               Good| 5000|2021-06-01| 48.32258065|         HR|\n+------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "a = spark.read.parquet(\"/FileStore/final_employee_data\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2a2b95-dafc-439f-b950-ec6ac1569284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "combining data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}