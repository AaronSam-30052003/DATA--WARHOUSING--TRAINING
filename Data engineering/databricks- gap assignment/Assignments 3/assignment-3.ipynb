{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace85739-2e28-43f8-a521-0dcb805b8619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Intialize the SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb6dd66-f1a0-469f-bdb5-dd3627d99fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3525126442407722#setting/sparkui/0611-043339-3vb7b9iv/driver-3407856474938293653\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7dc888870710>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    "      .appName(\"assignment-3\")\\\n",
    "      .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c3cf2f7-2c82-4622-b981-14ef74bd80c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data Ingestion & Schema Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc72948a-3bad-479c-80e3-7130ba7bf6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Project: string (nullable = true)\n |-- WorkHours: integer (nullable = true)\n |-- WorkDate: date (nullable = true)\n |-- Location: string (nullable = true)\n |-- Mode : string (nullable = true)\n\n+----------+-----+----------+-------+---------+----------+---------+-------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode |\n+----------+-----+----------+-------+---------+----------+---------+-------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|\n+----------+-----+----------+-------+---------+----------+---------+-------+\n\nroot\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Project: string (nullable = true)\n |-- WorkHours: integer (nullable = true)\n |-- WorkDate: date (nullable = true)\n |-- Location: string (nullable = true)\n |-- Mode: string (nullable = true)\n\n+----------+-----+----------+-------+---------+----------+---------+-------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|   Mode|\n+----------+-----+----------+-------+---------+----------+---------+-------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|\n+----------+-----+----------+-------+---------+----------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#1.Load the CSV using inferred schema.\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Project\", StringType(), True),\n",
    "    StructField(\"WorkHours\", IntegerType(), True),\n",
    "    StructField(\"WorkDate\", DateType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Mode\", StringType(), True)\n",
    "])\n",
    "inferred=spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/employee_timesheet.csv\", inferSchema=True)\n",
    "inferred.printSchema()\n",
    "inferred.show()\n",
    "#2.Load the same file with schema explicitly defined.\n",
    "emp=spark.read.option(\"header\", True).schema(schema).csv(\"file:/Workspace/Shared/employee_timesheet.csv\")\n",
    "emp.printSchema()\n",
    "emp.show()\n",
    "#3.Add a new column Weekday extracted from WorkDate.\n",
    "from pyspark.sql.functions import *\n",
    "weekday=emp.withColumn(\"Weekday\", date_format(col(\"WorkDate\"), \"EEEE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c85204-9493-45f0-9b72-d8349cbfeb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Aggregations & Grouping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd93ab9c-c750-4de2-941d-53ba43f8c2c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total work hours by employee:\n+----------+-----+----------+\n|EmployeeID| Name|TotalHours|\n+----------+-----+----------+\n|      E103| John|         5|\n|      E104|Meena|         6|\n|      E102|  Raj|        15|\n|      E101|Anita|        17|\n+----------+-----+----------+\n\naverage work hours per department:\n+----------+-----------------+\n|Department|         AvgHours|\n+----------+-----------------+\n|        HR|              7.5|\n|   Finance|              5.0|\n|        IT|7.666666666666667|\n+----------+-----------------+\n\n+----------+-----+----------+----+\n|EmployeeID| Name|TotalHours|Rank|\n+----------+-----+----------+----+\n|      E101|Anita|        17|   1|\n|      E102|  Raj|        15|   2|\n+----------+-----+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#4.Calculate total work hours by employee.\n",
    "print(\"total work hours by employee:\")\n",
    "emp.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).show()\n",
    "#5.Calculate average work hours per department.\n",
    "print(\"average work hours per department:\")\n",
    "emp.groupBy(\"Department\").agg(avg(\"WorkHours\").alias(\"AvgHours\")).show()\n",
    "#6.Get top 2 employees by total hours using window function.\n",
    "from pyspark.sql.window import Window\n",
    "total_hours=emp.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\"))\n",
    "w=Window.orderBy(desc(\"TotalHours\"))\n",
    "total_hours.withColumn(\"Rank\", row_number().over(w)).filter(\"Rank <= 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee916184-f4a5-421c-91bf-a0fb5ba7498e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Date Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b625e9c8-728f-4a07-bc32-d8f4b8f44b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries where WorkDate falls on a weekend:\n+----------+----+----------+-------+---------+----------+--------+------+\n|EmployeeID|Name|Department|Project|WorkHours|  WorkDate|Location|  Mode|\n+----------+----+----------+-------+---------+----------+--------+------+\n|      E102| Raj|        HR|   Beta|        8|2024-05-04|  Mumbai|Remote|\n+----------+----+----------+-------+---------+----------+--------+------+\n\nrunning total of hours per employee:\n+----------+-----+----------+-------+---------+----------+---------+-------+------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|   Mode|RunningTotal|\n+----------+-----+----------+-------+---------+----------+---------+-------+------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |           8|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |          17|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |           7|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|          15|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |           5|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |           6|\n+----------+-----+----------+-------+---------+----------+---------+-------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#7.Filter entries where WorkDate falls on a weekend.\n",
    "print(\"entries where WorkDate falls on a weekend:\")\n",
    "emp.filter(dayofweek(\"WorkDate\").isin([1, 7])).show()\n",
    "#8.Calculate running total of hours per employee using window.\n",
    "print(\"running total of hours per employee:\")\n",
    "windowEmp = Window.partitionBy(\"EmployeeID\").orderBy(\"WorkDate\")\n",
    "emp.withColumn(\"RunningTotal\",sum(\"WorkHours\").over(windowEmp)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab740af-71b2-4ed9-b670-fb97a6942910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Joining DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192c4e2b-4451-48cd-bad4-ee24ed37806a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+--------+\n|EmployeeID| Name|Department|DeptHead|\n+----------+-----+----------+--------+\n|      E101|Anita|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n|      E103| John|   Finance|   Kamal|\n|      E101|Anita|        IT|   Anand|\n|      E104|Meena|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n+----------+-----+----------+--------+\n\n+----------+-----+----+-----+\n|EmployeeID|Alpha|Beta|Gamma|\n+----------+-----+----+-----+\n|      E103|    5|NULL| NULL|\n|      E104| NULL|NULL|    6|\n|      E101|   17|NULL| NULL|\n|      E102| NULL|  15| NULL|\n+----------+-----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 9.Create department_location.csv\n",
    "dept=spark.createDataFrame([\n",
    "    (\"IT\", \"Anand\"),\n",
    "    (\"HR\", \"Shruti\"),\n",
    "    (\"Finance\", \"Kamal\")\n",
    "], [\"Department\", \"DeptHead\"])\n",
    "emp.join(dept, on=\"Department\", how=\"left\").select(\"EmployeeID\", \"Name\", \"Department\", \"DeptHead\").show()\n",
    "#10.Join with timesheet data and list all employees with their DeptHead.\n",
    "emp.groupBy(\"EmployeeID\").pivot(\"Project\").agg(sum(\"WorkHours\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9555d58b-f909-4b9f-a661-cbfaada0522a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Pivot & Unpivot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4532eb66-498e-4e36-b23b-625538e22801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+\n|EmployeeID|  Mode|ModeHours|\n+----------+------+---------+\n|      E103|Remote|        5|\n|      E104|Onsite|        6|\n|      E101|Remote|       17|\n|      E102|Remote|        8|\n|      E102|Onsite|        7|\n+----------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#11.Pivot table: total hours per employee per project.\n",
    "#12.Unpivot example: Convert mode-specific hours into rows.\n",
    "#Clean Mode values before pivoting\n",
    "cleaned_emp = emp.withColumn(\"Mode\", trim(col(\"Mode\")))\n",
    "#Pivot the cleaned data\n",
    "pivoted = cleaned_emp.groupBy(\"EmployeeID\").pivot(\"Mode\").agg(sum(\"WorkHours\"))\n",
    "#Rename columns to avoid ambiguity\n",
    "for col_name in pivoted.columns:\n",
    "    if col_name.strip() != col_name:\n",
    "        pivoted = pivoted.withColumnRenamed(col_name, col_name.strip())\n",
    "unpivot_expr = \"stack(2, 'Remote', Remote, 'Onsite', Onsite) as (Mode, ModeHours)\"\n",
    "pivoted.select(\"EmployeeID\", expr(unpivot_expr)).filter(\"ModeHours is not null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9752a950-2a1a-4717-acad-d108d5bd7f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**UDF & Conditional Logic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d71f2b-ec97-4082-af0a-9dc94fa3efb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+-------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|   Mode|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+-------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|            Full|\n+----------+-----+----------+-------+---------+----------+---------+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#13.Create a UDF to classify work hours\n",
    "def workload_tag(hours):\n",
    "    if hours >= 8:\n",
    "        return \"Full\"\n",
    "    elif hours >= 4:\n",
    "        return \"Partial\"\n",
    "    else:\n",
    "        return \"Light\"\n",
    "workload_udf = udf(workload_tag, StringType())\n",
    "#14.Add a column WorkloadCategory using this UDF\n",
    "emp=emp.withColumn(\"WorkloadCategory\", workload_udf(\"WorkHours\"))\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc1e6dc-89c0-415b-9bb6-37d5096fb857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Nulls and Cleanup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a469670-c092-46c2-b186-a39fd2e0a7db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduce some nulls in Mode column:\nfill nulls with 'Not Provided':\n+----------+-----+----------+-------+---------+----------+---------+------------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|     Remote |            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|     Onsite |         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|     Remote |         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|     Remote |            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Not Provided|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------------+----------------+\n\ndrop rows where WorkHours < 4:\n+----------+-----+----------+-------+---------+----------+---------+------------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|WorkloadCategory|\n+----------+-----+----------+-------+---------+----------+---------+------------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|     Remote |            Full|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|     Onsite |         Partial|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|     Remote |         Partial|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|     Remote |            Full|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Not Provided|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai|      Remote|            Full|\n+----------+-----+----------+-------+---------+----------+---------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#15. Introduce some nulls in Mode column.\n",
    "print(\"introduce some nulls in Mode column:\")\n",
    "nulls = emp.withColumn(\"Mode\", when(col(\"EmployeeID\") == \"E104\", None).otherwise(col(\"Mode\")))\n",
    "#16.Fill nulls with \"Not Provided\".\n",
    "print(\"fill nulls with 'Not Provided':\")\n",
    "filled=nulls.fillna({\"Mode\": \"Not Provided\"})\n",
    "filled.show()\n",
    "#17.Drop rows where WorkHours < 4.\n",
    "print(\"drop rows where WorkHours < 4:\")\n",
    "dropped=filled.filter(col(\"WorkHours\") >= 4)\n",
    "dropped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dab66c8-87f6-41f8-b177-b04441e5468f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Advanced Conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc449d0d-db46-4cb5-b24b-2dd0955a5472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n|EmployeeID|RemoteRatio|WorkerType|\n+----------+-----------+----------+\n|      E103|        0.0|     Mixed|\n|      E104|        0.0|     Mixed|\n|      E101|        0.0|     Mixed|\n|      E102|        0.5|     Mixed|\n+----------+-----------+----------+\n\nExtraHours where hours > 8:\n+----------+-----+----------+-------+---------+----------+---------+-------+----------------+----------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|   Mode|WorkloadCategory|ExtraHours|\n+----------+-----+----------+-------+---------+----------+---------+-------+----------------+----------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |            Full|         0|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |         Partial|         0|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |         Partial|         0|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |            Full|         1|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |         Partial|         0|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|            Full|         0|\n+----------+-----+----------+-------+---------+----------+---------+-------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#18.Use when-otherwise to mark employees as \"Remote Worker\" if >80% entries areRemote.\n",
    "ratio = emp.groupBy(\"EmployeeID\").agg(\n",
    "    (spark_sum(when(col(\"Mode\") == \"Remote\", 1).otherwise(0)) / count(\"*\")).alias(\"RemoteRatio\")\n",
    ")\n",
    "ratio.withColumn(\"WorkerType\", when(col(\"RemoteRatio\") > 0.8, \"Remote Worker\").otherwise(\"Mixed\")).show()\n",
    "#19.Add a new column ExtraHours where hours > 8\n",
    "emp=emp.withColumn(\"ExtraHours\", when(col(\"WorkHours\") > 8, col(\"WorkHours\") - 8).otherwise(0))\n",
    "print(\"ExtraHours where hours > 8:\")\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d132866b-fe43-46a6-a24f-31410ecea504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Union + Duplicate Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb9c426-23ac-4f6b-b925-0d2f19950bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+-------+---------+----------+---------+-------+----------------+----------+\n|EmployeeID|   Name|Department|Project|WorkHours|  WorkDate| Location|   Mode|WorkloadCategory|ExtraHours|\n+----------+-------+----------+-------+---------+----------+---------+-------+----------------+----------+\n|      E104|  Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |         Partial|         0|\n|      E103|   John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |         Partial|         0|\n|      E101|  Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |            Full|         0|\n|      E102|    Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |         Partial|         0|\n|      E102|    Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|            Full|         0|\n|      E101|  Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |            Full|         1|\n|      E200|Intern1|        IT|  Delta|        5|2024-05-05|Bangalore| Remote|           Light|         0|\n+----------+-------+----------+-------+---------+----------+---------+-------+----------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#20.Append a dummy timesheet for new interns using unionbyname()\n",
    "from datetime import date\n",
    "schema = schema.add(\"WorkloadCategory\", StringType(), True)\n",
    "schema = schema.add(\"ExtraHours\", IntegerType(), True)\n",
    "intern = spark.createDataFrame([\n",
    "    (\"E200\", \"Intern1\", \"IT\", \"Delta\", 5, date(2024, 5, 5), \"Bangalore\", \"Remote\", \"Light\", 0)\n",
    "], schema=schema)\n",
    "#21.Remove duplicate rows based on all columns.\n",
    "union = emp.unionByName(intern)\n",
    "union.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6211bd81-4ee9-457e-9afa-8831b013e228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "assignment-3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}