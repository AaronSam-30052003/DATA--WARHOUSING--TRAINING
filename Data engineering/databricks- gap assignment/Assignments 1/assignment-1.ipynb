{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97a5ac13-405a-4337-bec8-bfb77746fe65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Intialize the SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de9e087-1afe-40e2-b563-d9228a199485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3525126442407722#setting/sparkui/0611-043339-3vb7b9iv/driver-3407856474938293653\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7762a26e4790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    "      .appName(\"assignment-1\")\\\n",
    "      .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a8d6d5b-7f9f-436d-82d2-54e68206a288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**PySpark + Delta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83373e3f-4634-430b-b6a9-fa1d357f76ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- OrderID: integer (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- ProductID: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: integer (nullable = true)\n |-- OrderDate: date (nullable = true)\n |-- Status : string (nullable = true)\n\nroot\n |-- ProductID: string (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Stock: integer (nullable = true)\n |-- ReorderLevel : double (nullable = true)\n\nroot\n |-- CustomerID: string (nullable = true)\n |-- CustomerName: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- SignupDate : timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "# 1. Ingest all 3 CSVs as Delta Tables.\n",
    "# Read the data\n",
    "orders = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/orders.csv\", inferSchema=True)\n",
    "customers = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/customers.csv\", inferSchema=True)\n",
    "products = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/products.csv\", inferSchema=True)\n",
    "orders.printSchema()\n",
    "products.printSchema()\n",
    "customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0aecf53-d6ee-42ba-b55b-d4dfaec50cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    for old_name in df.columns:\n",
    "        df = df.withColumnRenamed(old_name, old_name.strip())\n",
    "    return df\n",
    "# Clean column names as they have some spaces\n",
    "orders = clean_column_names(orders)\n",
    "customers = clean_column_names(customers)\n",
    "products = clean_column_names(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f2aeb1-e5b0-49a6-8d1f-33f224ba63fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- OrderID: integer (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- ProductID: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: integer (nullable = true)\n |-- OrderDate: date (nullable = true)\n |-- Status: string (nullable = true)\n\nroot\n |-- CustomerID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Age: integer (nullable = true)\n\nroot\n |-- ProductID: string (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Stock: integer (nullable = true)\n |-- ReorderLevel: double (nullable = true)\n\nRevenue per Product:\n+---------+------------+\n|ProductID|TotalRevenue|\n+---------+------------+\n|    P1001|      150000|\n|    P1002|      150000|\n|    P1003|       30000|\n|    P1004|       30000|\n+---------+------------+\n\n+----+-----------+\n|City|CityRevenue|\n+----+-----------+\n+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Save it as delta table\n",
    "orders.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/orders\")\n",
    "customers.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/customers\")\n",
    "products.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/products\")\n",
    "#read the delta table\n",
    "orders = spark.read.format(\"delta\").load(\"file:/Workspace/Shared/orders\")\n",
    "customers = spark.read.format(\"delta\").load(\"file:/Workspace/Shared/customers\")\n",
    "products = spark.read.format(\"delta\").load(\"file:/Workspace/Shared/products\")\n",
    "orders.printSchema()\n",
    "customers.printSchema()\n",
    "products.printSchema()\n",
    "# 2. Write SQL to get the total revenue per Product.\n",
    "print(\"Revenue per Product:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ProductID,\n",
    "    SUM(Quantity * Price) AS TotalRevenue\n",
    "FROM delta.`file:/Workspace/Shared/orders`\n",
    "GROUP BY ProductID\n",
    "\"\"\").show()\n",
    "#3.Join Orders + Customers to find revenue by Region.\n",
    "orders.join(\n",
    "    customers.withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"string\")),\n",
    "    on=\"CustomerID\"\n",
    ").groupBy(\"City\") \\\n",
    " .agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"CityRevenue\")) \\\n",
    " .show()\n",
    "#4.Update the Status of Pending orders to 'Cancelled'.\n",
    "from delta.tables import DeltaTable\n",
    "delta_orders = DeltaTable.forPath(spark, \"file:/Workspace/Shared/orders\")\n",
    "delta_orders.update(\n",
    "    condition=\"Status = 'Pending'\",\n",
    "    set={\"Status\": \"'Cancelled'\"}\n",
    ")\n",
    "# 5.Merge a new return record into Orders.\n",
    "# Convert date string to datetime.date\n",
    "import datetime\n",
    "new_return=[(3006, \"C002\", \"P1001\", 1, 75000, datetime.datetime.strptime(\"2024-05-06\", \"%Y-%m-%d\").date(), \"Returned\")]\n",
    "new= spark.createDataFrame(new_return, schema=orders.schema)\n",
    "delta_orders.alias(\"t\").merge(\n",
    "    new.alias(\"s\"),\n",
    "    \"t.OrderID = s.OrderID\"\n",
    ").whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07de371-6982-4c76-b07c-006da0bd5b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**DLT Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c726898-eb08-4352-bed4-55b4e89e67b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.Create raw → cleaned → aggregated tables.\n",
    "# Cleaned table rows\n",
    "cleaned = orders.dropna()\n",
    "cleaned.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/clean_orders\")\n",
    "#Aggregated: Revenue per Category\n",
    "cleaned.join(products, \"ProductID\") \\\n",
    "       .groupBy(\"Category\") \\\n",
    "       .agg(sum(col(\"Quantity\") * col(\"Price\")).alias(\"CategoryRevenue\")) \\\n",
    "       .write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/category_revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f116071-397d-476d-9e65-7ec5f75faea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Time Travel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe2f48f-9e7f-49cf-8f61-d17934c3c3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.View data before the Status update.\n",
    "delta_orders = DeltaTable.forPath(spark, \"file:/Workspace/Shared/orders\")\n",
    "delta_orders.history()\n",
    "#8.Restore to Older Version\n",
    "delta_orders = DeltaTable.forPath(spark, \"file:/Workspace/Shared/orders\")\n",
    "delta_orders.restoreToVersion(0)\n",
    "delta_orders.history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eed4d4c3-abbf-46d5-805c-e08480a2c637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**VACUUM+RETENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1177b76e-24a3-49fe-9d91-15414672fb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9.VACUUM after changing default retention.\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
    "delta_orders.vacuum(1)\n",
    "delta_orders.history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80880554-04c0-463e-ab50-e33aad7e8117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Expectations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de7016e-edfa-4d4f-8581-ea9d89dd4c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+----------+----------+\n|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|    Status|\n+-------+----------+---------+--------+-----+----------+----------+\n|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered |\n|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned |\n|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered |\n|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered |\n|   3005|      C004|    P1004|       3|10000|2024-05-05|   Pending|\n+-------+----------+---------+--------+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10. Quantity > 0 , Price > 0 ,orderDate is not null\n",
    "delta_orders.toDF().filter((col(\"Quantity\") > 0) & (col(\"Price\") > 0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880feb16-fb2d-4d88-9154-465c39f05ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Bonus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8123d2-9128-42f3-9af4-6de7c0edd17b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------+-----+----------+----------+---------+\n|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|    Status|OrderType|\n+-------+----------+---------+--------+-----+----------+----------+---------+\n|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered |     Sale|\n|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned |     Sale|\n|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered |     Sale|\n|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered |     Sale|\n|   3005|      C004|    P1004|       3|10000|2024-05-05|   Pending|     Sale|\n+-------+----------+---------+--------+-----+----------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#11.Add 'OrderType' Column\n",
    "from pyspark.sql.functions import when\n",
    "orders.withColumn(\"OrderType\", when(col(\"Status\") == \"Returned\", \"Return\").otherwise(\"Sale\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4209b84c-8fb5-4246-9e58-8f844b924c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "assignment-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}